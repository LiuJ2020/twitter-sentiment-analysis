{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from preprocess.ipynb\n",
      "importing Jupyter notebook from stats.ipynb\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import import_ipynb\n",
    "from preprocess import *\n",
    "from stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe):\n",
    "    \n",
    "    stats = pd.DataFrame(columns=['Total', 'Unique', 'Average', 'Max', 'Positive', 'Negative'],\n",
    "                        index=['Tweets', 'User Mentions', 'Emoticons', 'URLs', 'Unigrams', 'Bigrams'])\n",
    "    \n",
    "    num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "    num_mentions, max_mentions = 0, 0\n",
    "    num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "    num_urls, max_urls = 0, 0\n",
    "    num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "    num_bigrams, num_unique_bigrams = 0, 0\n",
    "    all_words = []\n",
    "    all_bigrams = []\n",
    "    \n",
    "    #counting the number of total, positive, and negative tweets\n",
    "    num_tweets = len(dataframe)\n",
    "    for target in dataframe['TARGET']:\n",
    "        if target:\n",
    "            num_pos_tweets += 1\n",
    "        else:\n",
    "            num_neg_tweets += 1\n",
    "            \n",
    "    #analyze text properties\n",
    "    for tweet in dataframe['TEXT']:\n",
    "        result, words, bigrams = analyze_tweet(tweet)\n",
    "\n",
    "        #look at mentions\n",
    "        num_mentions += result['MENTIONS']\n",
    "        max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "\n",
    "        #look at emojis\n",
    "        num_pos_emojis += result['POS_EMOS']\n",
    "        num_neg_emojis += result['NEG_EMOS']\n",
    "        max_emojis = max(max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "\n",
    "        #look at URLs\n",
    "        num_urls += result['URLS']\n",
    "        max_urls = max(max_urls, result['URLS'])\n",
    "\n",
    "        #look at unigrams\n",
    "        num_words += result['WORDS']\n",
    "        min_words = min(min_words, result['WORDS'])\n",
    "        max_words = max(max_words, result['WORDS'])\n",
    "        all_words.extend(words)\n",
    "\n",
    "        #look at bigrams\n",
    "        num_bigrams += result['BIGRAMS']\n",
    "        all_bigrams.extend(bigrams)\n",
    "        \n",
    "    num_emojis = num_pos_emojis + num_neg_emojis\n",
    "\n",
    "    #find unique unigrams and bigrams\n",
    "    unique_words = list(set(all_words))\n",
    "    unique_bigrams = list(set(all_bigrams))\n",
    "\n",
    "    #count unique unigrams and bigrams\n",
    "    num_unique_words = len(unique_words)\n",
    "    num_unique_bigrams = len(all_bigrams)\n",
    "\n",
    "    #finding the frequency distribution for unigrams and bigrams\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "    \n",
    "    #calculate average of each per tweet\n",
    "    avg_mentions = num_mentions / num_tweets\n",
    "    avg_emojis = num_emojis / num_tweets\n",
    "    avg_urls = num_urls / num_tweets\n",
    "    avg_words = num_words / num_tweets\n",
    "    avg_bigrams = num_bigrams / num_tweets\n",
    "    \n",
    "    stats['Total'] = [num_tweets, num_mentions, num_emojis, num_urls, num_words, num_bigrams]\n",
    "    stats['Unique'] = ['-', '-', '-', '-', num_unique_words, num_unique_bigrams]\n",
    "    stats['Average'] = ['-', avg_mentions, avg_emojis, avg_urls, avg_words, avg_bigrams]\n",
    "    stats['Max'] = ['-', max_mentions, max_emojis, max_urls, max_words, '-']\n",
    "    stats['Positive'] = ['-', '-', num_pos_emojis, '-', '-', '-']\n",
    "    stats['Negative'] = ['-', '-', num_neg_emojis, '-', '-', '-']\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweets, unigrams, bigrams, VOCAB_SIZE, UNIGRAM_SIZE, USE_BIGRAMS, BATCH_SIZE=500):\n",
    "    \n",
    "    num_batches = int(np.ceil(len(tweets) / float(BATCH_SIZE)))\n",
    "    for i in range(num_batches):\n",
    "        batch = tweets[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "        features = lil_matrix((BATCH_SIZE, VOCAB_SIZE))\n",
    "        labels = np.zeros(BATCH_SIZE)\n",
    "        for j, tweet in enumerate(batch):\n",
    "            tweet_words = tweet[2][0]\n",
    "            tweet_bigrams = tweet[2][1]\n",
    "            labels[j] = tweet[1]\n",
    "            for word in tweet_words:\n",
    "                idx = unigrams.get(word)\n",
    "                if idx:\n",
    "                    features[j, idx] += 1\n",
    "            if USE_BIGRAMS:\n",
    "                for bigram in tweet_bigrams:\n",
    "                    idx = bigrams.get(bigram)\n",
    "                    if idx:\n",
    "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
    "        yield features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(dataframe, model_type, unigram_size=15000, use_bigrams=True, bigram_size=10000):\n",
    "    UNIGRAM_SIZE = unigram_size\n",
    "    VOCAB_SIZE = UNIGRAM_SIZE\n",
    "    USE_BIGRAMS = use_bigrams\n",
    "    if USE_BIGRAMS:\n",
    "        BIGRAM_SIZE = bigram_size\n",
    "        VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    \n",
    "    #get the list of unigrams/bigrams\n",
    "    unigrams = top_n_words(sample_data, UNIGRAM_SIZE)\n",
    "    if USE_BIGRAMS:\n",
    "        bigrams = top_n_bigrams(sample_data, BIGRAM_SIZE)\n",
    "    \n",
    "    #split this into the training data and test data\n",
    "    tweets = []\n",
    "    print('Generating feature vectors')\n",
    "    total = len(dataframe)\n",
    "    \n",
    "    \n",
    "    for information in dataframe.values:\n",
    "        tweet_id, sentiment, tweet = information\n",
    "        \n",
    "        #getting the feature vector\n",
    "        uni_feature_vector = []\n",
    "        bi_feature_vector = []\n",
    "        words = tweet.split()\n",
    "        #adding each word in the tweet and the word ahead of it for bigrams\n",
    "        for i in range(len(words) - 1):\n",
    "            word = words[i]\n",
    "            next_word = words[i + 1]\n",
    "            if unigrams.get(word):\n",
    "                uni_feature_vector.append(word)\n",
    "            if USE_BIGRAMS:\n",
    "                if bigrams.get((word, next_word)):\n",
    "                    bi_feature_vector.append((word, next_word))\n",
    "        #adding last word to make sure it isn't left out\n",
    "        if len(words) >= 1:\n",
    "            if unigrams.get(words[-1]):\n",
    "                uni_feature_vector.append(words[-1])\n",
    "        feature_vector = uni_feature_vector, bi_feature_vector\n",
    "        \n",
    "        sentiment = 1 if sentiment else 0\n",
    "        tweets.append((tweet_id, int(sentiment), feature_vector))\n",
    "    print('\\n')\n",
    "    \n",
    "    train_tweets, val_tweets = split_data(tweets)\n",
    "    del tweets\n",
    "    \n",
    "    print('Extracting features & training batches')\n",
    "    \n",
    "    #initialize the model according to type given by user\n",
    "    if model_type is 'NAIVEBAYES':\n",
    "        clf = MultinomialNB()\n",
    "    elif model_type is 'LOGISTICREGRESSION':\n",
    "        clf = LogisticRegression()\n",
    "    elif model_type is 'DECISIONTREE':\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif model_type is 'RANDOMFOREST':\n",
    "        clf = RandomForestClassifier()\n",
    "    else:\n",
    "        raise NameError(model_type + ' is not a valid model type.')\n",
    "    \n",
    "    batch_size = len(train_tweets)\n",
    "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
    "    \n",
    "    #fit the model\n",
    "    for training_set_X, training_set_y in extract_features(val_tweets, unigrams, bigrams, VOCAB_SIZE, UNIGRAM_SIZE, USE_BIGRAMS, BATCH_SIZE=batch_size):\n",
    "        tfidf = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "        tfidf.fit(training_set_X)\n",
    "        training_set_X = tfidf.transform(training_set_X)\n",
    "    clf.fit(training_set_X, training_set_y)\n",
    "    \n",
    "    #determine accuracy of model\n",
    "    correct, total = 0, len(val_tweets)\n",
    "    batch_size = len(val_tweets)\n",
    "    n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
    "    for val_set_X, val_set_y in extract_features(val_tweets, unigrams, bigrams, VOCAB_SIZE, UNIGRAM_SIZE, USE_BIGRAMS, BATCH_SIZE=batch_size):\n",
    "        val_set_X = tfidf.transform(val_set_X)\n",
    "        prediction = clf.predict(val_set_X)\n",
    "        correct += np.sum(prediction == val_set_y)\n",
    "    print('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
    "    \n",
    "    #creating confusion matrix\n",
    "    con_mat = confusion_matrix(prediction, val_set_y)\n",
    "    \n",
    "    acc_table = pd.DataFrame(data=con_mat, columns=['Actually Negative', 'Actually Positive'], index=['Classified As Negative', 'Classified As Positive'])\n",
    "    \n",
    "    return correct, total, acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"\"\"/Users/jacobliu/Documents/Schoolwork/Computer Science/Projects/TweetSentimentAnalysis/training.1600000.processed.noemoticon.csv\"\"\", \n",
    "                   names=['TARGET', 'ID', 'TIMESTAMP', 'FLAG', 'USER', 'TEXT'],\n",
    "                   encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>919796</th>\n",
       "      <td>1753884557</td>\n",
       "      <td>True</td>\n",
       "      <td>heaos keen for next weekend mummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258778</th>\n",
       "      <td>1985270993</td>\n",
       "      <td>False</td>\n",
       "      <td>USER_MENTION i feel all the way round everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865590</th>\n",
       "      <td>1677489256</td>\n",
       "      <td>True</td>\n",
       "      <td>just visited with god and mr god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849630</th>\n",
       "      <td>1565165223</td>\n",
       "      <td>True</td>\n",
       "      <td>USER_MENTION USER_MENTION wow sounds exciting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163699</th>\n",
       "      <td>1958106109</td>\n",
       "      <td>False</td>\n",
       "      <td>wish i was wembley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  TARGET                                               TEXT\n",
       "919796  1753884557    True                  heaos keen for next weekend mummy\n",
       "258778  1985270993   False  USER_MENTION i feel all the way round everythi...\n",
       "865590  1677489256    True                   just visited with god and mr god\n",
       "849630  1565165223    True  USER_MENTION USER_MENTION wow sounds exciting ...\n",
       "163699  1958106109   False                                 wish i was wembley"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = preprocess_dataframe(data, slice_value=1, positive_value=4)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "\n",
      "\n",
      "Extracting features & training batches\n",
      "\n",
      "Correct: 79989/160000 = 49.9931 %\n",
      "Generating feature vectors\n",
      "\n",
      "\n",
      "Extracting features & training batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobliu/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correct: 116610/160000 = 72.8812 %\n",
      "Generating feature vectors\n",
      "\n",
      "\n",
      "Extracting features & training batches\n",
      "\n",
      "Correct: 159126/160000 = 99.4537 %\n"
     ]
    }
   ],
   "source": [
    "correct_bayes, total_bayes, acc_table_bayes = model_data(sample_data, 'NAIVEBAYES')\n",
    "correct_tree, total_tree, acc_table_tree = model_data(sample_data, 'LOGISTICREGRESSION')\n",
    "correct_for, total_for, acc_table_for = model_data(sample_data, 'DECISIONTREE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
